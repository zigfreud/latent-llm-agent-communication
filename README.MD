<div align="center">

ğŸ§  LIP: Latent Injection Protocol (Research Lab)

ğŸ§ª Universal Latent Injection Protocol for Heterogeneous Model Communication

"O mundo inteiro estÃ¡ se perguntando como organizar LLMs de forma eficiente. Mas por que ninguÃ©m fala sobre ensinÃ¡-las a conversar em uma lÃ­ngua eficiente?"

Este projeto explora a comunicaÃ§Ã£o direta via espaÃ§o latente, eliminando a serializaÃ§Ã£o de texto intermediÃ¡ria.

</div>

ğŸ“‹ Resumo do Projeto

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o oficial, dados experimentais e cÃ³digo-fonte do paper "LIP: A Universal Latent Injection Protocol for Heterogeneous Model Communication".

A arquitetura atual de Sistemas Multi-Agente (MAS) Ã© ineficiente:
Agente A (Pensa) -> Texto -> TokenizaÃ§Ã£o -> Agente B (LÃª)

Nossa Proposta (V2V):
Agente A (Pensa) -> Vetor Latente -> Adaptador LIP -> Agente B (Entende)

O LIP permite que LLMs (mesmo de arquiteturas diferentes, como TinyLlama e Llama-3) se comuniquem diretamente via vetores, possibilitando:

Privacidade: Ovetores sÃ£o ilegÃ­veis para humanos (obfuscaÃ§Ã£o natural).

EficiÃªncia: Elimina a etapa de decodificaÃ§Ã£o/geraÃ§Ã£o de texto intermediÃ¡rio.

ComputaÃ§Ã£o DistribuÃ­da: Ideal para arquiteturas Edge-to-Cloud.

ğŸ—ï¸ Arquitetura do Experimento (Gen 4)

Nesta versÃ£o (v2), validamos a comunicaÃ§Ã£o HeterogÃªnea. Utilizamos um TinyLlama-1.1B (Cliente/Sender) comandando um Llama-3-8B (Servidor/Receiver).

O pipeline consiste em trÃªs estÃ¡gios:

Sender (TinyLlama): Processa a instruÃ§Ã£o e exporta o last_hidden_state (o "pensamento vetorial").

Projector (Adaptador LIP): Uma rede neural treinada (Dual-Encoder) que alinha o espaÃ§o latente do modelo menor ao do modelo maior.

Receiver (Llama-3): Recebe o vetor projetado diretamente em seus inputs_embeds e executa a aÃ§Ã£o.

graph TD;
User[User Prompt] -->|Texto| AgentA(Sender: TinyLlama 1.1B);
AgentA -->|Last Hidden State| Adapter{LIP Adapter / Projector};
Adapter -->|Vetor Alinhado| AgentB(Receiver: Llama-3 8B);
AgentB -->|ExecuÃ§Ã£o| Output[CÃ³digo/Resposta];

    style AgentA fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    style AgentB fill:#e3f2fd,stroke:#01579b,stroke-width:2px
    style Adapter fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,stroke-dasharray: 5 5

ğŸ“‚ Estrutura do RepositÃ³rio

Este laboratÃ³rio estÃ¡ organizado para garantir a reprodutibilidade cientÃ­fica:

.
â”œâ”€â”€ paper/ # O Artigo CientÃ­fico (PDF, LaTeX, Figuras)
â”œâ”€â”€ src/ # CÃ³digo Fonte do Protocolo e Demos
â”‚ â”œâ”€â”€ lip_protocol.py # A definiÃ§Ã£o do Pacote LIP (JSON + Tensores)
â”‚ â””â”€â”€ lip_integration.py # Demo Cliente-Servidor (TinyLlama -> Llama-3)
â”œâ”€â”€ experiments/ # Scripts de ReproduÃ§Ã£o CientÃ­fica
â”‚ â”œâ”€â”€ train.py # Script de Treino do Adaptador (Gen 4)
â”‚ â”œâ”€â”€ dataset_generator.py # FÃ¡brica de Vetores (Filtra Alpaca)
â”‚ â””â”€â”€ benchmarks/ # Scripts que geraram os grÃ¡ficos do paper
â”œâ”€â”€ models/ # Checkpoints dos Adaptadores Treinados (.pth)
â””â”€â”€ datasets/ # Datasets de vetores prÃ©-processados (.pt)

ğŸš€ Guia de InÃ­cio RÃ¡pido (Quick Start)

1. PrÃ©-requisitos

Python 3.10+

Hardware: 16GB RAM (CPU-Only) ou GPU com 12GB+ VRAM.

# Clone o repositÃ³rio

git clone [https://github.com/zigfreud/latent-llm-agent-communication.git](https://github.com/zigfreud/latent-llm-agent-communication.git)
cd latent-llm-agent-communication

# Crie um ambiente virtual

python -m venv .venv
source .venv/bin/activate # Linux/Mac

# ou .venv\Scripts\activate no Windows

# Instale as dependÃªncias

pip install torch transformers accelerate datasets matplotlib pandas numpy

2. Executando a Demo (Telepatia IA)

Esta demo carrega o TinyLlama (Sender) e o Llama-3 (Receiver) e demonstra a transferÃªncia de instruÃ§Ãµes via vetor.

Certifique-se de possuir o arquivo de pesos lip_hetero_adapter_final.pth na pasta models/.

python src/lip_integration.py

ğŸ”¬ Reproduzindo a CiÃªncia

Siga este pipeline para treinar o adaptador do zero e reproduzir os resultados da Gen 4 descritos no paper.

Passo 1: Gerar o Dataset de Vetores

Baixa o dataset Alpaca, filtra instruÃ§Ãµes de cÃ³digo e extrai/alinha os vetores dos dois modelos (Source e Target).

python experiments/dataset_generator.py

# SaÃ­da: Cria datasets/dataset_code_only.pt

Passo 2: Treinar o Adaptador

Treina a rede neural (Dual-Encoder) para alinhar os espaÃ§os latentes.

python experiments/train.py

# SaÃ­da: Salva models/lip_hetero_adapter_final.pth e logs

ğŸ“Š Resultados Preliminares (Gen 4)

Comparativo com versÃµes anteriores (PoC):

MÃ©trica

Status

ObservaÃ§Ã£o

Viabilidade TÃ©cnica

âœ… Sucesso

ConvergÃªncia de Loss < 0.003. O modelo aceita injeÃ§Ã£o vetorial heterogÃªnea.

TransferÃªncia de Sintaxe

âœ… Sucesso

O Receiver (Llama-3) gera cÃ³digo Python vÃ¡lido a partir de "pensamentos" do TinyLlama.

Integridade SemÃ¢ntica

âœ… EstÃ¡vel

Ao contrÃ¡rio da PoC (que sofria Mode Collapse), a Gen 4 atingiu estabilidade semÃ¢ntica robusta na validaÃ§Ã£o.

ğŸ“ CitaÃ§Ã£o / Citation

Se vocÃª utilizar o LIP (Latent Injection Protocol) em sua pesquisa ou projeto, por favor cite da seguinte forma:

BibTeX (LaTeX)

@article{silva2025lip,
title={LIP: A Universal Latent Injection Protocol for Heterogeneous Model Communication},
author={Silva, Cristiano},
year={2025},
publisher={GitHub},
url={[https://github.com/zigfreud/latent-llm-agent-communication](https://github.com/zigfreud/latent-llm-agent-communication)}
}
