<div align="center">

# ğŸ§¬ LIP â€” Latent Injection Protocol

### Why speak in JSON when models can think in vectors?

</div>

---

## Status

**Active Research Prototype (Gen6 â€” stabilization phase)**

This repository contains **experimental research code**.  
Architectures, APIs, and results are subject to change.

The project is currently transitioning from **Gen4 â†’ Gen6**, with recent
breakthroughs in **correct hidden-state injection mechanics** and **deterministic
AB validation**.

---

## ğŸš€ The Thesis

> *â€œThe field is focused on organizing LLMs via orchestration frameworks.
But little attention is given to teaching models to communicate in an efficient,
native language.â€*

Modern Multi-Agent Systems (MAS) suffer from what we call the **Dinner Table Paradox**:
highly capable agents share the same environment, yet are forced to communicate
through **human-centric channels** (text / JSON).

This introduces structural bottlenecks:

- **Latency** â€” repeated tokenization and decoding waste compute.
- **Bandwidth** â€” large contexts (e.g., RAG) saturate communication channels.
- **Leakage risk** â€” semantic intent is flattened into plaintext.
- **Semantic loss** â€” rich latent structure is discarded at every hop.

---

## âœ¨ Core Idea â€” Direct Vector-to-Vector Communication

LIP explores **direct latent communication between heterogeneous LLMs**.

Instead of exchanging tokens, a source model transmits **semantic vectors**
that are translated and injected directly into the internal computation of
a target model.

Example setup:

- **Edge / Source:** TinyLlama or DeepSeek-Coder (2048d)
- **Cloud / Target:** Llama-3-8B (4096d)
- **Bridge:** Asymmetric adapter with a bottleneck (e.g., 2048 â†’ 512 â†’ 4096)

This enables **functional steering** of the target model **without textual exchange**.

---

## âš¡ What Works (Gen6)

- âœ… **Heterogeneous latent translation** (2048d â†’ 4096d)
- âœ… **Contrastive alignment (InfoNCE-style)** outperforms MSE in high-dimensional spaces
- âœ… **Correct hidden-state injection** (not input embeddings)
- âœ… Stable injection via **forward hooks** into the **penultimate transformer layer (-2)**
- âœ… **Deterministic AB testing** confirms non-placebo steering
- âœ… Functional control demonstrated in code-generation tasks
  (e.g., explanation â†’ implementation)

---

## ğŸ”¬ Key Technical Decisions (Empirically Validated)

- **Injection target:** Hidden states (never input embeddings)
- **Layer:** Penultimate transformer layer (-2)
- **Spatial focus:** Final token position
- **Temporal scope:** Single injection at initial forward pass
- **Calibration:** Norm-based energy matching relative to target embeddings

These choices were validated through ablations on layer, position, and gain.

---

## âš ï¸ Known Limitations

- âŒ Fine-grained intent transfer remains inconsistent  
  (vectors often encode *mode/style* better than precise constraints)
- âŒ Dataset currently aligned to final-layer targets while injection occurs at layer -2
  (being corrected)
- âŒ Evaluation is limited to controlled prompts
- âŒ No standardized benchmarks yet
- âŒ 8B inference is **cloud-only** for stability reasons

These limitations are explicitly acknowledged and align with open problems
identified in related work.

---

## ğŸ§ª Reproducibility

This repository contains a **script-based, config-driven pipeline** (no notebooks):

src/
core/ # models, losses
pipelines/ # extract, train, infer
integrations/ # injection hooks
configs/ # yaml experiment configs
datasets/ # sharded latent pairs
checkpoints/ # adapter weights
scripts/ # CLI entry points


Experiments are designed to be **deterministic and inspectable**.

---

## ğŸ›£ï¸ Roadmap (Near-Term)

- Align dataset mining with **layer-specific injection**
- Add **quantitative retrieval metrics** (top-k, cosine vs baseline)
- Ablations on:
  - injection layer
  - token position
  - gain / blending strength
- Small-scale benchmark tasks (code, reasoning, structured output)

---

## ğŸ” Privacy & Security (Research Scope)

LIP currently provides **obfuscation by design**:
vectors are non-human-readable and reduce accidental leakage.

Planned research phases:

1. **Current:** Obfuscation by design âœ…  
2. **Near-term:** Secure vector transport (TLS-like protocol for packets)  
3. **Long-term:** Partial homomorphic routing (research-only, limited scope)

No cryptographic security claims are made at this stage.

---

## ğŸ¤ Contribution Policy

This project is **not yet open for general contributions**.

Feedback, inspection, and discussion are welcome.
Targeted collaboration may be enabled once the Gen6 pipeline is fully stabilized.

---

## ğŸ“ Disclaimer

This is **research code**, not a production library.

Expect breaking changes.

---

<div align="center">

_An Independent Research Project by  
[Cristiano Silva](https://www.linkedin.com/in/cristiano-silva-a2a084204/)_

</div>

---

## ğŸ“š Citation

If you reference this work:

Silva, Cristiano. "LIP â€” Latent Injection Protocol". 2025.
GitHub repository: https://github.com/zigfreud/latent-llm-agent-communication