{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Instala√ß√£o das depend√™ncias e preparo do ambiente."
      ],
      "metadata": {
        "id": "LM-n4JO8X9h3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOnVvnsp58rr"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caso receba erro de import das bibliotecas, √© s√≥ atualizar:"
      ],
      "metadata": {
        "id": "djLydIJNX7em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "gbBEEAHMX6Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiro, precisamos entender se √© poss√≠vel enviar um hidden_state diretamente como input de um modelo.\n",
        "Premissas:\n",
        "1. Os dois modelos devem ser exatamente iguais, para que tenhamos outputs compat√≠veis entre eles.\n",
        "2. Foi necess√°rio quantizar os modelos para rodar localmente no Colab.\n",
        "3. Foi necess√°rio converter as matrizes de pensamento todas para o tipo de sa√≠da do modelo (bfloat16) para garantir compatibilidade."
      ],
      "metadata": {
        "id": "KJyeG-H7SRdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Usando uma vers√£o acess√≠vel do Llama 3 Instruct para n√£o lidar com autentica√ß√£o exigida pelo modelo oficial\n",
        "model_id = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "print(f\"Carregando {model_id} (Isso pode demorar um pouco)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Llama 3 Carregado. Preparando inje√ß√£o V2V...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- PREPARA√á√ÉO DO PENSAMENTO (Agente Emissor) ---\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Escreva uma fun√ß√£o Python eficiente que receba uma lista e retorne apenas os n√∫meros pares. Apenas o c√≥digo.\"}\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Extra√ß√£o do Vetor\n",
        "    vector_thought = model.get_input_embeddings()(input_ids)\n",
        "\n",
        "    print(f\"1. Instru√ß√£o Vetorial Criada. Shape: {vector_thought.shape}\")\n",
        "\n",
        "    # --- O EXPERIMENTO V2V (Agente Receptor) ---\n",
        "    print(\"2. Injetando pensamento no Agente Receptor...\")\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        inputs_embeds=vector_thought, # O modelo recebe o vetor, n√£o o texto.\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"RESPOSTA DO AGENTE (Gerada via Vetor):\")\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "dHof1wpm6Fh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A esse ponto, descobri que, sim, **√© poss√≠vel** enviar hidden states diretamente como inputs para modelos de linguagem, sem a necessidade da √∫ltima etapa de serializa√ß√£o para texto.\n",
        "\n",
        "> RESPOSTA DO AGENTE (Gerada via Vetor):\n",
        "\n",
        "```\n",
        "> def pares(lista):\n",
        "return [i for i in lista if i % 2 == 0]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2SJt_AbzTSB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fase 2: Capturar e enviar o embedding antes que o modelo o transforme em texto.\n",
        "# Objetivo: Agente A processa uma info e passa o contexto para o Agente B continuar.\n",
        "\n",
        "print(\"Iniciando Fase 2: Transfer√™ncia de Estado Oculto (Hidden State)...\")\n",
        "\n",
        "# 1. O AGENTE \"PLANEJADOR\" (Vector)\n",
        "prompt_agente_a = [\n",
        "    {\"role\": \"user\", \"content\": \"Analise o seguinte problema: Criar uma API de login. Liste os 3 passos t√©cnicos principais. Seja breve.\"}\n",
        "]\n",
        "input_ids_a = tokenizer.apply_chat_template(prompt_agente_a, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_a = model(input_ids_a, output_hidden_states=True)\n",
        "\n",
        "    # Pegamos o √∫ltimo estado oculto .\n",
        "    # Shape: [Batch, Sequence Length, Hidden Size] (Ex: [1, 45, 4096])\n",
        "    last_hidden_state_a = outputs_a.hidden_states[-1]\n",
        "\n",
        "    print(f\"Agente A 'pensou'. Tamanho do pensamento: {last_hidden_state_a.shape}\")\n",
        "\n",
        "    # Num sistema real, enviar√≠amos esse tensor pela rede/mem√≥ria compartilhada.\n",
        "    # Aqui, vamos passar direto para o Agente B.\n",
        "\n",
        "    # --- O AGENTE \"CODER\" (Receptor) ---\n",
        "    # O Agente B recebe o pensamento do A e deve continuar a partir dali.\n",
        "    # Ele n√£o deve recer o input original em string. Ele v√™ apenas o output embedded do A.\n",
        "\n",
        "    # Projetamos o hidden_state de volta para o espa√ßo de embedding ou usamos como contexto?\n",
        "    # Para o primeiro teste, vamos tentar usar o hidden state como input_embeds.\n",
        "    # *Nota Te√≥rica:* Isso geralmente requer um adaptador, mas vamos testar a robustez do Llama 3 \"cru\".\n",
        "\n",
        "    print(\"Injetando racioc√≠nio do Agente A no Agente B...\")\n",
        "\n",
        "    generation_b = model.generate(\n",
        "        inputs_embeds=last_hidden_state_a,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    output_text_b = tokenizer.decode(generation_b[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"O QUE O AGENTE B GEROU:\")\n",
        "print(output_text_b)"
      ],
      "metadata": {
        "id": "BwysIxr-_tHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como esperado, n√£o seria t√£o simples. √â necess√°rio usar um normalizador para que o output esteja no mesmo \"formato\" do input esperado pelo Llama.\n",
        "\n",
        "*Output:*\n",
        "\n",
        "```\n",
        "O QUE O AGENTE B GEROU:\n",
        " toalalsicalalalischalichtalal...\n",
        "\n",
        "ichalaln duplex:alichtalific autor...\n",
        "\n",
        "ich, atried to create to be anal of simple technique to carry the firsts, carrying,alisch to: the firstln'tal, return,alischal to the firstalitzals of the last, a dearzichals for treatment at the time. (ried:ried, and return toisch to the brain, withou: a virtual, and\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ePInPQ0BZKCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 5\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "min_delta = 0.0010\n",
        "\n",
        "training_data = [\n",
        "    {\"instruction\": \"Escreva uma fun√ß√£o soma em Python.\", \"target_code\": \"def soma(a, b): return a + b\"},\n",
        "    {\"instruction\": \"Crie uma lista de n√∫meros pares at√© 10.\", \"target_code\": \"pares = [x for x in range(11) if x % 2 == 0]\"},\n",
        "    {\"instruction\": \"Imprima 'Ola Mundo' em Python.\", \"target_code\": \"print('Ola Mundo')\"},\n",
        "    {\"instruction\": \"Fun√ß√£o para multiplicar dois valores.\", \"target_code\": \"def mult(x, y): return x * y\"}\n",
        "]\n",
        "\n",
        "# Adaptador\n",
        "class VectorAdapter(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(VectorAdapter, self).__init__()\n",
        "        self.projector = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_float = x.to(torch.float32)\n",
        "        out = self.projector(x_float)\n",
        "        return out.to(torch.float16)\n",
        "\n",
        "adapter = VectorAdapter(4096, 4096).to(\"cuda\")\n",
        "optimizer = optim.AdamW(adapter.parameters(), lr=5e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Iniciando Treino (Paci√™ncia: {patience})...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "epochs = 200\n",
        "loss_history = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for item in training_data:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Input do Agente A\n",
        "        messages_a = [{\"role\": \"user\", \"content\": item['instruction']}]\n",
        "        input_a = tokenizer.apply_chat_template(messages_a, return_tensors=\"pt\").to(\"cuda\")\n",
        "        target_tokens = tokenizer(item['target_code'], return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\").input_ids\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_a = model(input_a, output_hidden_states=True)\n",
        "            thought_vector = outputs_a.hidden_states[-1][:, -1, :]\n",
        "            target_embeddings = model.get_input_embeddings()(target_tokens)\n",
        "\n",
        "        # Para acelerar o teste das hip√≥teses, estamos tentando adivinhar somente o primeiro token da resposta.\n",
        "        # Por isso, concatenamos com o restante da resposta desejada, e vamos medir a acur√°cia em cima disso.\n",
        "        adapted_vector = adapter(thought_vector).unsqueeze(1)\n",
        "        inputs_seq = torch.cat([adapted_vector, target_embeddings[:, :-1, :]], dim=1)\n",
        "\n",
        "        outputs_b = model(inputs_embeds=inputs_seq)\n",
        "        logits = outputs_b.logits\n",
        "        shift_logits = logits.view(-1, model.config.vocab_size)\n",
        "        shift_labels = target_tokens.view(-1)\n",
        "        loss = loss_fn(shift_logits, shift_labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(adapter.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(training_data)\n",
        "\n",
        "    if avg_loss < (best_loss - min_delta):\n",
        "        best_loss = avg_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"√âpoca {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Paci√™ncia: {patience_counter}/{patience}\")\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\n‚úã PARADA ANTECIPADA na √©poca {epoch+1}.\")\n",
        "        print(f\"Melhor Loss alcan√ßada: {best_loss:.4f}\")\n",
        "        break\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "test_instruction = \"Escreva uma fun√ß√£o soma em Python.\"\n",
        "print(f\"Testando instru√ß√£o: '{test_instruction}'\")\n",
        "msg_test = [{\"role\": \"user\", \"content\": test_instruction}]\n",
        "input_test = tokenizer.apply_chat_template(msg_test, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_test = model(input_test, output_hidden_states=True)\n",
        "    thought_test = out_test.hidden_states[-1][:, -1, :]\n",
        "    projected_thought = adapter(thought_test).unsqueeze(1)\n",
        "    gen = model.generate(\n",
        "        inputs_embeds=projected_thought,\n",
        "        max_new_tokens=30,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "    res = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Resposta V2V: {res}\")"
      ],
      "metadata": {
        "id": "ZUYC7GJX_7b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A inten√ß√£o do primeiro token (devido nosso dataset ser extremamente simples) ainda √© muito forte, ent√£o alucinamos.\n",
        "\n",
        "Em vez de usar uma regress√£o linear, vamos construir adicionar uma camada de rede neural pra capturar o restante do contexto."
      ],
      "metadata": {
        "id": "J_aVXiq5an4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Limpeza de VRAM.\")\n",
        "\n",
        "# Congelamos os pesos pra n√£o estourar a VRAM.\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "training_data = [\n",
        "    {\"instruction\": \"Escreva uma fun√ß√£o soma em Python.\", \"target_code\": \"def soma(a, b): return a + b\"},\n",
        "    {\"instruction\": \"Crie uma lista de n√∫meros pares at√© 10.\", \"target_code\": \"pares = [x for x in range(11) if x % 2 == 0]\"},\n",
        "    {\"instruction\": \"Imprima 'Ola Mundo' em Python.\", \"target_code\": \"print('Ola Mundo')\"},\n",
        "    {\"instruction\": \"Fun√ß√£o para multiplicar dois valores.\", \"target_code\": \"def mult(x, y): return x * y\"}\n",
        "]\n",
        "\n",
        "class MLPAdapter(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(MLPAdapter, self).__init__()\n",
        "\n",
        "        # Reduzimos a camada oculta para 1024 (Economiza vram e ainda √© n√£o-linear)\n",
        "        hidden_dim = 1024\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_float = x.to(torch.float32)\n",
        "        out = self.net(x_float)\n",
        "        return out.to(torch.float16)\n",
        "\n",
        "adapter = MLPAdapter(4096, 4096).to(\"cuda\")\n",
        "optimizer = optim.AdamW(adapter.parameters(), lr=2e-4, weight_decay=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def evaluate_latent_alignment(model, adapter, test_data, tokenizer):\n",
        "    \"\"\"\n",
        "    Avalia o qu√£o pr√≥ximo o vetor projetado pelo Agente A est√°\n",
        "    do vetor de embedding real do Agente B (Target).\n",
        "\n",
        "    Retorna: Similaridade de Cosseno M√©dia (-1 a 1).\n",
        "    Quanto mais pr√≥ximo de 1, melhor o alinhamento sem√¢ntico.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    adapter.eval()\n",
        "\n",
        "    similarities = []\n",
        "\n",
        "    print(\"\\n--- üìê Avalia√ß√£o de Geometria Latente ---\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for item in test_data:\n",
        "            instruction = item['instruction']\n",
        "            target_code = item['target_code']\n",
        "\n",
        "            msg_a = [{\"role\": \"user\", \"content\": instruction}]\n",
        "            input_ids_a = tokenizer.apply_chat_template(msg_a, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "            outputs_a = model(input_ids_a, output_hidden_states=True)\n",
        "            thought_vector_a = outputs_a.hidden_states[-1][:, -1, :]\n",
        "            projected_vector = adapter(thought_vector_a)\n",
        "\n",
        "            target_tokens = tokenizer(target_code, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\").input_ids\n",
        "            first_token_target = target_tokens[:, 0].unsqueeze(0)\n",
        "            target_embedding_b = model.get_input_embeddings()(first_token_target).squeeze(1)\n",
        "\n",
        "            cos_sim = F.cosine_similarity(projected_vector, target_embedding_b, dim=-1)\n",
        "            similarities.append(cos_sim.item())\n",
        "\n",
        "            print(f\"Instru√ß√£o: {instruction[:30]}... | Sim: {cos_sim.item():.4f}\")\n",
        "\n",
        "    avg_sim = sum(similarities) / len(similarities)\n",
        "    print(f\"üìä Similaridade M√©dia do Espa√ßo Latente: {avg_sim:.4f}\")\n",
        "    print(\"-----------------------------------------\\n\")\n",
        "\n",
        "    return avg_sim\n",
        "\n",
        "\n",
        "print(\"Adaptador MLP minimizado inicializado.\")\n",
        "print(\"Iniciando Treino...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "epochs = 150\n",
        "loss_history = []\n",
        "\n",
        "model.eval()\n",
        "aux_loss_fn = nn.MSELoss()\n",
        "lambda_align = 10.0\n",
        "\n",
        "print(\"Iniciando Treino com Loss H√≠brida (Texto + Geometria)...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_align_loss = 0\n",
        "    adapter.train()\n",
        "\n",
        "    for item in training_data:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            messages_a = [{\"role\": \"user\", \"content\": item['instruction']}]\n",
        "            input_a = tokenizer.apply_chat_template(messages_a, return_tensors=\"pt\").to(\"cuda\")\n",
        "            target_tokens = tokenizer(item['target_code'], return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\").input_ids\n",
        "\n",
        "            outputs_a = model(input_a, output_hidden_states=True)\n",
        "            thought_vector = outputs_a.hidden_states[-1][:, -1, :]\n",
        "\n",
        "            # Queremos que o output do adaptador se pare√ßa com o embedding do 1¬∫ token da resposta\n",
        "            first_token_target = target_tokens[:, 0].unsqueeze(0)\n",
        "            target_embedding_b = model.get_input_embeddings()(first_token_target).squeeze(1) # [1, 4096]\n",
        "\n",
        "            # Para o texto, continuamos precisando da sequ√™ncia completa\n",
        "            target_embeddings_seq = model.get_input_embeddings()(target_tokens)\n",
        "\n",
        "        # O adaptador gera o vetor\n",
        "        adapted_vector = adapter(thought_vector)\n",
        "\n",
        "        # For√ßamos 'adapted_vector' a ser igual a 'target_embedding_b'\n",
        "        loss_align = aux_loss_fn(adapted_vector.float(), target_embedding_b.float())\n",
        "\n",
        "        # Continua o fluxo para o decoder (Loss de Texto)\n",
        "        adapted_vector_seq = adapted_vector.unsqueeze(1)\n",
        "        inputs_seq = torch.cat([adapted_vector_seq, target_embeddings_seq[:, :-1, :]], dim=1)\n",
        "\n",
        "        outputs_b = model(inputs_embeds=inputs_seq)\n",
        "        logits = outputs_b.logits\n",
        "        shift_logits = logits.view(-1, model.config.vocab_size)\n",
        "        shift_labels = target_tokens.view(-1)\n",
        "\n",
        "        loss_text = loss_fn(shift_logits, shift_labels)\n",
        "        total_loss = loss_text + (lambda_align * loss_align)\n",
        "\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(adapter.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += total_loss.item()\n",
        "        epoch_align_loss += loss_align.item()\n",
        "\n",
        "if (epoch + 1) % 20 == 0:\n",
        "        evaluate_latent_alignment(model, adapter, training_data, tokenizer)\n",
        "        mse_medio = epoch_align_loss / len(training_data)\n",
        "        print(f\"üîç Diagn√≥stico de Loss:\")\n",
        "        print(f\"   - Loss Geometria (MSE): {mse_medio:.6f} (Peso: {lambda_align})\")\n",
        "        print(f\"   - Loss Total (H√≠brida): {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "test_instruction = \"Escreva uma fun√ß√£o soma em Python.\"\n",
        "print(f\"Testando: '{test_instruction}'\")\n",
        "\n",
        "msg_test = [{\"role\": \"user\", \"content\": test_instruction}]\n",
        "input_test = tokenizer.apply_chat_template(msg_test, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_test = model(input_test, output_hidden_states=True)\n",
        "    thought_test = out_test.hidden_states[-1][:, -1, :]\n",
        "    projected_thought = adapter(thought_test).unsqueeze(1)\n",
        "    gen = model.generate(\n",
        "        inputs_embeds=projected_thought,\n",
        "        max_new_tokens=40,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "    res = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Resposta do nosso V2V: {res}\")"
      ],
      "metadata": {
        "id": "9_w_Xe5AOvuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estamos rodando em \"modo econ√¥mico\", ent√£o √© praticamente esperado que ele entre em loop."
      ],
      "metadata": {
        "id": "cunMYswIcO2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TESTE DE RESGATE (Infer√™ncia Calibrada) ---\n",
        "print(\"Testando com par√¢metros Anti-Loop Agressivos...\")\n",
        "\n",
        "test_instruction = \"Escreva uma fun√ß√£o soma em Python.\"\n",
        "msg_test = [{\"role\": \"user\", \"content\": test_instruction}]\n",
        "input_test = tokenizer.apply_chat_template(msg_test, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_a = model(input_test, output_hidden_states=True)\n",
        "    thought_test = outputs_a.hidden_states[-1][:, -1, :]\n",
        "\n",
        "    projected_thought = adapter(thought_test).unsqueeze(1)\n",
        "\n",
        "    print(f\"Instru√ß√£o: '{test_instruction}'\")\n",
        "    print(\"Gerando...\")\n",
        "\n",
        "    gen = model.generate(\n",
        "        inputs_embeds=projected_thought,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        repetition_penalty=2.5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    res = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"Nova resposta V2V: {res}\")"
      ],
      "metadata": {
        "id": "O09N2pOeRHxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conseguimos fugir do loop do primeiro token \"defdefdef\", pelo menos."
      ],
      "metadata": {
        "id": "l74Q9VAIcm4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Teste de Dire√ß√£o Sem√¢ntica...\")\n",
        "instrucao_soma = \"Escreva uma fun√ß√£o soma em Python.\"\n",
        "instrucao_mult = \"Fun√ß√£o para multiplicar dois valores.\"\n",
        "\n",
        "\n",
        "for inst in [instrucao_soma, instrucao_mult]:\n",
        "    msg = [{\"role\": \"user\", \"content\": inst}]\n",
        "    inp = tokenizer.apply_chat_template(msg, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        thought = model(inp, output_hidden_states=True).hidden_states[-1][:, -1, :]\n",
        "        vec = adapter(thought).unsqueeze(1)\n",
        "\n",
        "        # O objetivo aqui √© validar se ele recebeu a inten√ß√£o do prompt inicial,\n",
        "        # ou se s√≥ t√° escrevendo def \"aleatoriamente\"; ent√£o fazemos o contr√°rio do que fizemos l√° em cima:\n",
        "        # Passamos o def e deixamos ele completar o resto do c√≥digo.\n",
        "        start_token = tokenizer(\"def \", return_tensors=\"pt\").to(\"cuda\").input_ids\n",
        "\n",
        "        # Input = [vetor] + [\"def\"]\n",
        "        inputs_seq = torch.cat([vec, model.get_input_embeddings()(start_token)], dim=1)\n",
        "        gen = model.generate(\n",
        "            inputs_embeds=inputs_seq,\n",
        "            max_new_tokens=20,\n",
        "            do_sample=False\n",
        "        )\n",
        "        res = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\nInstru√ß√£o: {inst}\")\n",
        "    print(f\"Resultado: {res}\")"
      ],
      "metadata": {
        "id": "-EiGXMpTRfx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para essa etapa do estudo, podemos considerar um **sucesso**:\n",
        "1. O modelo entendeu que deve gerar c√≥digo Python; mas ainda n√£o capturou a inten√ß√£o do c√≥digo (somar vs multiplica√ß√£o).\n",
        "\n",
        "Nosso pr√≥ximo passo √© balancear os pesos e fazer com que o modelo consiga diferenciar os prompts simples.\n",
        "Depois disso, vamos elevando a complexidade dos prompts (que vai precisar de mais recursos de hardware pra funcionar; vamos ter que diminuir as quantiza√ß√µes e diminui√ß√µes que fizemos).\n",
        "\n",
        "# O Resultado?\n",
        "\n",
        "\n",
        "> **Para criarmos sistemas Multi-Agentes de LLM, n√£o precisamos necessariamente da camada de serializa√ß√£o para texto intermedi√°ria.**\n",
        "\n",
        "\n",
        "√â *poss√≠vel* criar um protocolo de comunica√ß√£o que envie inten√ß√µes latentes entre os modelos/agentes ao inv√©s de texto puro.\n",
        "Mas **precisamos** construir um dataset de treino efetivo para validar a generaliza√ß√£o e persist√™ncia sem√¢ntica."
      ],
      "metadata": {
        "id": "y7UA5lH7dQJW"
      }
    }
  ]
}